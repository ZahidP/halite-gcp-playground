{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Functions and State Shapes\n",
    "\n",
    "### Intro\n",
    "For me, one of the more interesting parts of this competition is how the reward functions and modified state shapes can effect an agent's ability to perform well.\n",
    "\n",
    "Some of this flies in the face of the entire purpose of reinforcement learning. In the case of rewards I would like to present this snippet: \"taken from Richard Sutton and Andrew Barto's intro book on Reinforcement Learning:\n",
    "\n",
    "> The reward signal is your way of communicating to the [agent] what you want it to achieve, not how you want it achieved (author emphasis).\n",
    ">For example, a chess-playing agent should be rewarded only for actually winning, not for achieving subgoals such as taking its opponents pieces or gaining control of the center.\"\n",
    "\n",
    "Additionally,\n",
    "\n",
    ">Newcomers to reinforcement learning are sometimes surprised that the rewards—which define of the goal of learning—are computed in the environment rather than in the agent...\n",
    "\n",
    ">For example, if the goal concerns a robot’s internal energy reservoirs, then these are considered to\n",
    "be part of the environment; if the goal concerns the positions of the robot’s limbs, then these too are considered to be part of the environment—that is, the agent’s boundary is drawn at the interface between the limbs and their\n",
    "control systems. These things are considered internal to the robot but external to the learning agent. \n",
    "\n",
    "The simplest reward function would be 1 for winning and 0 for everything else.\n",
    "\n",
    "### Motivation\n",
    "I kept running into the problem (especially while training against the random agents) of my agents deciding the best thing to do would be to do nothing.\n",
    "\n",
    "Against the random agent this makes sense. Generally speaking the random agent will keep spawning new agents or converting to shipyards (reducing its total score). In this scenario the player agent is content to just sit back and not spend halite converting or spawning if it doesn't need to do so.\n",
    "\n",
    "### Strategies\n",
    "\n",
    "#### Find Better Agents to Play Against\n",
    "This approach would at least result in games that my agent would lose sometimes. We still run into this issue of the agent only being rewarded at the end of a game. \n",
    "\n",
    "This doesn't mean that the agent only learns at the end of a game, but it does mean that many-many full games need to be played out if my only reward is winning or losing. \n",
    "\n",
    "\n",
    "\n",
    "#### Reward Shaping\n",
    "See below.\n",
    "\n",
    "### Reward Shaping\n",
    "\n",
    "From [Andrew Y. Ng, Daishi Harada, Stuart Russell],\n",
    ">  These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known bugs\" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoalbased heuristics. We show that such potentials can lead to substantial reductions in learning time.\n",
    "\n",
    "Additionally from this write-up,\n",
    "https://medium.com/@BonsaiAI/deep-reinforcement-learning-models-tips-tricks-for-writing-reward-functions-a84fe525e8e0\n",
    "> You want to instead shape rewards that get gradual feedback and let it know it’s getting better and getting closer. It helps it learn a lot faster\n",
    "\n",
    "The focus of this notebook is on reward shaping. The goal is to see if we can nudge the agents to learn a bit faster and perhaps with better agents, we can train the final agent _against_ those agents such that it actually has to react to learn good moves. \n",
    "\n",
    "The idea here is to incentivize some sort of intermediate reward that typically leads to winning. What we don't want to do, is overspecify in a way that results in more or less writing a rule-based approach.\n",
    "\n",
    "#### Reward Shaping Ideas: Ships\n",
    "\n",
    "- (COLLECT/DEPOSIT) Total halite for that ship plus total halite for player\n",
    "   - One drawback is that this ship would never try to CONVERT because it does not improve either objective\n",
    "- (ATTACK) Subtract total halite mined by opponent ships\n",
    "   - I have a feeling this would take a long time for the ship to learn that it should attack another ship if it has less halite than it\n",
    "- (CONVERT) Reward a particular ship to shipyard ratio, or penalize ships with too much mined halite\n",
    "   \n",
    "#### Reward Shaping Ideas: Shipyards\n",
    "\n",
    "- Reward the shipyard agents for achieving parity in ships between the player and the opponents\n",
    "- Reward shipyards for ensuring that there are at least X ships per Y halite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "code_dir = os.environ.get('HALITE_PATH')\n",
    "\n",
    "if not code_dir:\n",
    "    code_dir = '/'.join(os.getcwd().split('/')[:-1] + ['code'])\n",
    "sys.path.append(code_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from kaggle_environments import make\n",
    "from kaggle_environments.envs.halite.helpers import Board, ShipAction, ShipyardAction, Observation\n",
    "\n",
    "from halite_env import HaliteEnv\n",
    "from ship_state_wrapper import ShipStateWrapper\n",
    "from shipyard_state_wrapper import ShipYardStateWrapper\n",
    "from agent import Agent\n",
    "from game_runner_v2 import GameRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_with_reward(reward_type, max_steps, episodes):\n",
    "    ship_frame_stack_len = 2\n",
    "    env = make(\"halite\", debug=True)\n",
    "    ship_state_wrapper = ShipStateWrapper(\n",
    "        radius=4,\n",
    "        max_frames=ship_frame_stack_len,\n",
    "        map_size=int(env.configuration['size'])\n",
    "    )\n",
    "\n",
    "    shipyard_state_wrapper = ShipYardStateWrapper(\n",
    "        radius=4,\n",
    "        max_frames=1,\n",
    "        map_size=int(env.configuration['size'])\n",
    "    )\n",
    "\n",
    "    print(env.configuration)\n",
    "\n",
    "    print(\"Initialized state wrappers\")\n",
    "\n",
    "    ship_agent = Agent(\n",
    "        alpha=0.99, gamma=0.5, n_actions=6,\n",
    "        batch_size=32, epsilon=.9, input_dims=ship_state_wrapper.state_size\n",
    "    )\n",
    "\n",
    "    shipyard_agent = Agent(\n",
    "        alpha=0.99, gamma=0.5, n_actions=2,\n",
    "        batch_size=32, epsilon=.9, input_dims=shipyard_state_wrapper.state_size\n",
    "    )\n",
    "\n",
    "    print(\"Initialized agents\")\n",
    "    \n",
    "    players = [None, \"random\"]\n",
    "\n",
    "    trainer = env.train(players)\n",
    "\n",
    "    print(\"Initialized trainer\")\n",
    "    \n",
    "    halite_env = HaliteEnv(\n",
    "        environment=env,\n",
    "        opponents=len(players),\n",
    "        ship_state_wrapper=ship_state_wrapper,\n",
    "        shipyard_state_wrapper=shipyard_state_wrapper,\n",
    "        radius=4,\n",
    "        trainer=trainer,\n",
    "        ship_reward_type=reward_type\n",
    "    ) \n",
    "    \n",
    "    game = GameRunner(\n",
    "        configuration=env.configuration,\n",
    "        env=halite_env,\n",
    "        ship_agent=ship_agent,\n",
    "        shipyard_agent=shipyard_agent,\n",
    "        training=True,\n",
    "        ship_frame_stack_len=ship_frame_stack_len\n",
    "    )\n",
    "    \n",
    "    all_scores = []\n",
    "    for episode in range(episodes):\n",
    "        scores = game.play_episode(max_steps)\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    return {\n",
    "        'all_scores': all_scores,\n",
    "        'ship_agent': ship_agent,\n",
    "        'shipyard_agent': shipyard_agent,\n",
    "        'env': env\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Shaping A\n",
    "\n",
    "Our first attempt at reward shaping will encourage ships to collect and deposit halite. \n",
    "\n",
    "- Here we will deduct points at each timestep.\n",
    "- We will also add points for the difference between the previous amount of halite the ship had and the current amount of halite it has. \n",
    "- In order to encourage an increase in player halite instead of just ship halite, we will multiply the new ship halite by 0.5.\n",
    "- Finally, we will add the difference in total player halite. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_loss_reward(observation):\n",
    "    player_halite = observation.players[observation.player][0]\n",
    "    opponent_halites = [item[0] for item in observation.players[observation.player:]]\n",
    "    best_opponent_halite = sorted(opponent_halites, reverse=True)[0]\n",
    "\n",
    "    if player_halite > best_opponent_halite:\n",
    "        return 500\n",
    "    else:\n",
    "        return -500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_training_with_reward('total_halite', 150, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = results['all_scores']\n",
    "ship_agent = results['ship_agent']\n",
    "shipyard_agent = results['shipyard_agent']\n",
    "env = results['env']\n",
    "rewards = all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for episode in rewards[-5:]:\n",
    "    i += 1\n",
    "    episode = np.array(episode)\n",
    "    for j in range(episode.shape[1]):\n",
    "        plt.subplot(3, 2, i)\n",
    "        sns.lineplot(x=range(0, episode.shape[0]), y=episode[:, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode=\"ipython\",width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_training_with_reward('basic', 150, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = results['all_scores']\n",
    "ship_agent = results['ship_agent']\n",
    "shipyard_agent = results['shipyard_agent']\n",
    "env = results['env']\n",
    "rewards = all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for episode in rewards[-5:]:\n",
    "    i += 1\n",
    "    episode = np.array(episode)\n",
    "    for j in range(episode.shape[1]):\n",
    "        plt.subplot(3, 2, i)\n",
    "        sns.lineplot(x=range(0, episode.shape[0]), y=episode[:, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode=\"ipython\",width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_training_with_reward('collector', 150, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = results['all_scores']\n",
    "ship_agent = results['ship_agent']\n",
    "shipyard_agent = results['shipyard_agent']\n",
    "env = results['env']\n",
    "rewards = all_scores\n",
    "\n",
    "i = 0\n",
    "for episode in rewards[-5:]:\n",
    "    i += 1\n",
    "    episode = np.array(episode)\n",
    "    for j in range(episode.shape[1]):\n",
    "        plt.subplot(3, 2, i)\n",
    "        sns.lineplot(x=range(0, episode.shape[0]), y=episode[:, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode=\"ipython\",width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
