{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Functions and State Shapes\n",
    "\n",
    "### Intro\n",
    "For me, one of the more interesting parts of this competition is how the reward functions and modified state shapes can effect an agent's ability to perform well.\n",
    "\n",
    "Some of this flies in the face of the entire purpose of reinforcement learning. In the case of rewards I would like to present this snippet: \"taken from Richard Sutton and Andrew Barto's intro book on Reinforcement Learning:\n",
    "\n",
    "> The reward signal is your way of communicating to the [agent] what you want it to achieve, not how you want it achieved (author emphasis).\n",
    ">For example, a chess-playing agent should be rewarded only for actually winning, not for achieving subgoals such as taking its opponents pieces or gaining control of the center.\"\n",
    "\n",
    "Additionally,\n",
    "\n",
    ">Newcomers to reinforcement learning are sometimes surprised that the rewards—which define of the goal of learning—are computed in the environment rather than in the agent...\n",
    "\n",
    ">For example, if the goal concerns a robot’s internal energy reservoirs, then these are considered to\n",
    "be part of the environment; if the goal concerns the positions of the robot’s limbs, then these too are considered to be part of the environment—that is, the agent’s boundary is drawn at the interface between the limbs and their\n",
    "control systems. These things are considered internal to the robot but external to the learning agent. \n",
    "\n",
    "The simplest reward function would be 1 for winning and 0 for everything else.\n",
    "\n",
    "### Motivation\n",
    "I kept running into the problem (especially while training against the random agents) of my agents deciding the best thing to do would be to do nothing.\n",
    "\n",
    "Against the random agent this makes sense. Generally speaking the random agent will keep spawning new agents or converting to shipyards (reducing its total score). In this scenario the player agent is content to just sit back and not spend halite converting or spawning if it doesn't need to do so.\n",
    "\n",
    "### Strategies\n",
    "\n",
    "#### Make it possible to lose games\n",
    "It is very important to either have an opponent increase their halite, or have the player decrease their halite (artificially). This will remove the incentive for the agent to sit around until it inevitably wins.\n",
    "\n",
    "- improve the opponent agent\n",
    "- by default subtract -N halite when the game starts (problem is that this effects ability to spawn/convert)\n",
    "\n",
    "#### Reward Shaping\n",
    "See below.\n",
    "\n",
    "### Reward Shaping\n",
    "\n",
    "From [Andrew Y. Ng, Daishi Harada, Stuart Russell],\n",
    ">  These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known bugs\" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoalbased heuristics. We show that such potentials can lead to substantial reductions in learning time.\n",
    "\n",
    "Additionally from this write-up,\n",
    "https://medium.com/@BonsaiAI/deep-reinforcement-learning-models-tips-tricks-for-writing-reward-functions-a84fe525e8e0\n",
    "> You want to instead shape rewards that get gradual feedback and let it know it’s getting better and getting closer. It helps it learn a lot faster\n",
    "\n",
    "The focus of this notebook is on reward shaping. The goal is to see if we can nudge the agents to learn a bit faster and perhaps with better agents, we can train the final agent _against_ those agents such that it actually has to react to learn good moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
